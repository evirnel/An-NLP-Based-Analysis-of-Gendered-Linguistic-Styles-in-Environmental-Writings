{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f060df0-4adb-46fc-a714-8e0f14baaf36",
   "metadata": {},
   "source": [
    "## PCFG Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa47ad4-5ceb-44ea-bd6e-7906c41e0012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV shape: (12, 3)\n",
      "Number of unique authors: 12\n",
      "Total samples: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/emmavirnelli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/emmavirnelli/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.parse import ChartParser\n",
    "from nltk import PCFG, ViterbiParser\n",
    "from collections import Counter\n",
    "nltk.download(['punkt', 'averaged_perceptron_tagger'])\n",
    "\n",
    "df = pd.read_csv(\"NLPCleanData.csv\")\n",
    "print(f\"CSV shape: {df.shape}\")\n",
    "print(f\"Number of unique authors: {df['Author'].nunique()}\")\n",
    "print(f\"Total samples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb05d42-9c7f-43bf-8dcf-c19cdf56f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode thhe gender column\n",
    "le = LabelEncoder()\n",
    "df['Gender'] = le.fit_transform(df['Gender'])  # Now Male=0, Female=1\n",
    "\n",
    "# Seperate texts by gender\n",
    "male_texts = df[df['Gender'] == 0]['Sample'].tolist()\n",
    "female_texts = df[df['Gender'] == 1]['Sample'].tolist()\n",
    "\n",
    "all_text = male_texts + female_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30874e20-36d7-451d-af43-b02f9bb35803",
   "metadata": {},
   "source": [
    "This step extracts the grammar rules directly from our dataset so that the rules reflect the actual patterns used by male and female authors in nature writing. Instead of using generic grammar rules, we're building custom rules based on how these specific authors structure their sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "006711ec-de94-4cdc-8b59-540136384fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grammar from you data in order to find what patterns exists\n",
    "# Look for what words appear (according to POS tagging and Penn Treebank)\n",
    "\n",
    "def find_grammar_rules(texts, max_words=20):\n",
    "    # Extract all words with POS tags\n",
    "    all_tagged = []\n",
    "    for text in texts:\n",
    "        sentences = nltk.sent_tokenize(str(text))\n",
    "        for sent in sentences:\n",
    "            words = nltk.word_tokenize(sent)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            all_tagged.extend(tagged)\n",
    "    \n",
    "    # Group words by POS category\n",
    "    categories = {\n",
    "        'N': [],  # Nouns\n",
    "        'V': [],  # Verbs\n",
    "        'Adj': [],  # Adjectives\n",
    "        'Adv': [],  # Adverbs\n",
    "        'Det': [],  # Determiners\n",
    "    }\n",
    "    \n",
    "    # POS to category mapping, reference Penn Treebank to corrrectly identify each category\n",
    "    # https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html \n",
    "    pos_mapping = {\n",
    "        'NN': 'N', 'NNS': 'N', 'NNP': 'N', 'NNPS': 'N', # Noun (singular or mass), plural noun, proper nouns (both singular and  plural)\n",
    "        'VB': 'V', 'VBD': 'V', 'VBG': 'V', 'VBN': 'V', 'VBP': 'V', 'VBZ': 'V', # Verb (base and past form), gerund/present participle, past participle, non-, 3rd person singualr person\n",
    "        'JJ': 'Adj', 'JJR': 'Adj', 'JJS': 'Adj', # Adjectives,  comparative and superlative adjectives \n",
    "        'RB': 'Adv', 'RBR': 'Adv', 'RBS': 'Adv', # Adverb, comparative and superlative adverbs\n",
    "        'DT': 'Det'} # Determiner \n",
    "    \n",
    "    # Define stopwords\n",
    "    stopwords = {'to', 'from', 'not', 'my', 'so', 'do', 'one', \n",
    "                 'the', 'of', 'and', 'in', 'a', 'on', 'or',\n",
    "                 'as', 'that', 'there', 'about', 'up', 'no',\n",
    "                 'was', 'is', 'are', 'were', 'had', 'have', 'has',\n",
    "                 'be', 'been', 'this', 'all', 'an', 's', 't',\n",
    "                 'more', 'other', 'only', 'very', 'most', 'than',\n",
    "                 'also', 'any', 'can', 'could', 'would', 'should',\n",
    "                 'may', 'might', 'must', 'will', 'shall'}\n",
    "    \n",
    "    # Collect words for each category with filtering \n",
    "    word_counts_by_category = {cat: Counter() for cat in categories}\n",
    "    \n",
    "    for word, pos in all_tagged:\n",
    "        word_lower = word.lower()\n",
    "        \n",
    "        if (len(word_lower) <= 2 or \n",
    "            word_lower in stopwords or\n",
    "            not word_lower.isalpha()):\n",
    "            continue\n",
    "            \n",
    "        if pos in pos_mapping:\n",
    "            category = pos_mapping[pos]\n",
    "            word_counts_by_category[category][word_lower] += 1\n",
    "            if word_lower not in categories[category]:\n",
    "                categories[category].append(word_lower)\n",
    "    \n",
    "    print(\"Top Words:\")\n",
    "    for category in ['N', 'V', 'Adj', 'Adv', 'Det']:\n",
    "        counter = word_counts_by_category[category]\n",
    "        if counter:\n",
    "            top_words = counter.most_common(max_words)\n",
    "            print(f\"\\n{category} (Top {len(top_words)}):\")\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                print(f\"  {i:2d}. {word:<15} ({count})\")\n",
    "    \n",
    "    print(\"Counts of Each Category:\")\n",
    "    for cat, counter in word_counts_by_category.items():\n",
    "        total_words = sum(counter.values())\n",
    "        unique_words = len(counter)\n",
    "        print(f\"  {cat}: {total_words} total, {unique_words} unique\")\n",
    "    \n",
    "    # Get most frequent words \n",
    "    word_counts = Counter([word.lower() for word, _ in all_tagged \n",
    "                          if word.lower().isalpha() and len(word.lower()) > 2])\n",
    "    \n",
    "    # Build PCFG with probabilities\n",
    "    grammar_rules = []\n",
    "\n",
    "    # Update the already made categories with the referenced probabilities\n",
    "    # https://gawron.sdsu.edu/compling/course_core/lectures/pcfg/prob_parse.htm\n",
    "    \n",
    "    # NP + VP (most common pattern) and probability must add up to 1\n",
    "    grammar_rules.append(\"S -> NP VP [1.0]\")\n",
    "    grammar_rules.append(\"NP -> Det N [0.4] | N [0.4] | Det Adj N [0.2]\")\n",
    "    grammar_rules.append(\"VP -> V [0.3] | V NP [0.4] | V NP PP [0.3]\")\n",
    "    \n",
    "    # Prepositional phrase always contains P + NP\n",
    "    grammar_rules.append(\"PP -> P NP [1.0]\")\n",
    "    \n",
    "    \n",
    "    # Calculate total counts for each category to normalize probabilities\n",
    "    # This sums up all word occurrences within each POS category\n",
    "    category_totals = {}\n",
    "    for cat in ['N', 'V', 'Adj', 'Adv', 'Det']:\n",
    "        category_totals[cat] = sum(word_counts_by_category[cat].values())\n",
    "    \n",
    "    # Add words for each category \n",
    "    for category in ['N', 'V', 'Adj', 'Adv', 'Det']:\n",
    "        if categories[category]:\n",
    "            # Get top words by frequency\n",
    "            cat_words = [w for w in categories[category] \n",
    "                       if w in word_counts and w not in stopwords]\n",
    "            cat_words.sort(key=lambda w: word_counts[w], reverse=True)\n",
    "            top_words = cat_words[:max_words]\n",
    "            \n",
    "            if top_words:\n",
    "                # Calculate probabilities based on word frequencies in this category\n",
    "                rule_parts = []\n",
    "                total_in_category = category_totals.get(category, 1)  # Avoid division by zero for no erros\n",
    "                \n",
    "                for word in top_words:\n",
    "                    word_count = word_counts_by_category[category].get(word, 0)\n",
    "                    if total_in_category > 0:\n",
    "                        # Raw probability = frequency in category\n",
    "                        raw_prob = word_count / total_in_category\n",
    "                        rule_parts.append((word, raw_prob))\n",
    "                \n",
    "                # Normalize so all probabilities for this rule sum to 1, all expansions of a non-terminal must sum to 1\n",
    "                total_prob = sum(prob for _, prob in rule_parts)\n",
    "                \n",
    "                if total_prob > 0:\n",
    "                    normalized_parts = []\n",
    "                    for word, prob in rule_parts:\n",
    "                        normalized_prob = prob / total_prob\n",
    "                        # Round to 4 decimal places for readability\n",
    "                        normalized_parts.append(f\"'{word}' [{normalized_prob:.4f}]\")\n",
    "                    \n",
    "                    rule_str = \" | \".join(normalized_parts)\n",
    "                    grammar_rules.append(f\"{category} -> {rule_str}\")\n",
    "                else:\n",
    "                    # If we don't have frequency data, just split the probability evenly\n",
    "                    equal_prob = 1.0 / len(top_words)\n",
    "                    probs = [f\"'{w}' [{equal_prob:.4f}]\" for w in top_words]\n",
    "                    rule_str = \" | \".join(probs)\n",
    "                    grammar_rules.append(f\"{category} -> {rule_str}\")\n",
    "\n",
    "    \n",
    "    # Prepositions and conjuncstions based on how often they show up in English, based on my own bias and reference\n",
    "    grammar_rules.append(\n",
    "        \"P -> 'in' [0.15] | 'on' [0.10] | 'at' [0.08] | 'with' [0.08] | \"\n",
    "        \"'for' [0.10] | 'to' [0.12] | 'from' [0.07] | 'of' [0.20] | 'by' [0.05] | 'about' [0.05]\"\n",
    "    )\n",
    "    \n",
    "    # Conjunctions with approximate frequencies, 'and' dominates due to usage\n",
    "    grammar_rules.append(\"Conj -> 'and' [0.7] | 'or' [0.2] | 'but' [0.1]\")\n",
    "    \n",
    "    grammar_str = \"\\n\".join(grammar_rules)\n",
    "    \n",
    "    print(f\"({len(grammar_rules)} grammar rules total):\")\n",
    "    print(grammar_str)\n",
    "    \n",
    "    # Return as PCFG \n",
    "    from nltk import PCFG\n",
    "    return PCFG.fromstring(grammar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5599ab7-c11b-44dd-8f4c-4a9e60fbc0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Words:\n",
      "\n",
      "N (Top 20):\n",
      "   1. time            (112)\n",
      "   2. years           (98)\n",
      "   3. world           (73)\n",
      "   4. people          (72)\n",
      "   5. black           (69)\n",
      "   6. water           (68)\n",
      "   7. life            (64)\n",
      "   8. day             (63)\n",
      "   9. buffaloes       (62)\n",
      "  10. insects         (61)\n",
      "  11. animals         (60)\n",
      "  12. way             (58)\n",
      "  13. colours         (57)\n",
      "  14. country         (56)\n",
      "  15. herd            (55)\n",
      "  16. flies           (55)\n",
      "  17. house           (51)\n",
      "  18. river           (51)\n",
      "  19. light           (49)\n",
      "  20. year            (47)\n",
      "\n",
      "V (Top 20):\n",
      "   1. being           (68)\n",
      "   2. found           (61)\n",
      "   3. made            (57)\n",
      "   4. get             (48)\n",
      "   5. did             (45)\n",
      "   6. make            (45)\n",
      "   7. know            (43)\n",
      "   8. came            (41)\n",
      "   9. seen            (41)\n",
      "  10. got             (38)\n",
      "  11. see             (38)\n",
      "  12. come            (36)\n",
      "  13. used            (36)\n",
      "  14. killed          (34)\n",
      "  15. think           (34)\n",
      "  16. take            (33)\n",
      "  17. want            (33)\n",
      "  18. went            (32)\n",
      "  19. known           (32)\n",
      "  20. took            (30)\n",
      "\n",
      "Adj (Top 20):\n",
      "   1. many            (100)\n",
      "   2. few             (71)\n",
      "   3. white           (68)\n",
      "   4. such            (63)\n",
      "   5. old             (61)\n",
      "   6. same            (60)\n",
      "   7. great           (54)\n",
      "   8. small           (50)\n",
      "   9. little          (46)\n",
      "  10. own             (43)\n",
      "  11. green           (43)\n",
      "  12. young           (42)\n",
      "  13. large           (42)\n",
      "  14. different       (40)\n",
      "  15. wild            (35)\n",
      "  16. good            (35)\n",
      "  17. human           (34)\n",
      "  18. first           (33)\n",
      "  19. new             (32)\n",
      "  20. native          (29)\n",
      "\n",
      "Adv (Top 20):\n",
      "   1. then            (87)\n",
      "   2. just            (87)\n",
      "   3. now             (75)\n",
      "   4. even            (64)\n",
      "   5. well            (60)\n",
      "   6. never           (56)\n",
      "   7. here            (54)\n",
      "   8. sometimes       (53)\n",
      "   9. still           (49)\n",
      "  10. often           (48)\n",
      "  11. too             (45)\n",
      "  12. again           (45)\n",
      "  13. almost          (45)\n",
      "  14. once            (43)\n",
      "  15. away            (43)\n",
      "  16. back            (40)\n",
      "  17. always          (40)\n",
      "  18. soon            (36)\n",
      "  19. however         (35)\n",
      "  20. yet             (33)\n",
      "\n",
      "Det (Top 9):\n",
      "   1. these           (154)\n",
      "   2. some            (148)\n",
      "   3. every           (54)\n",
      "   4. those           (48)\n",
      "   5. another         (46)\n",
      "   6. both            (42)\n",
      "   7. each            (37)\n",
      "   8. either          (14)\n",
      "   9. neither         (4)\n",
      "Counts of Each Category:\n",
      "  N: 14849 total, 4660 unique\n",
      "  V: 6572 total, 2303 unique\n",
      "  Adj: 4456 total, 1317 unique\n",
      "  Adv: 2466 total, 430 unique\n",
      "  Det: 547 total, 9 unique\n",
      "(11 grammar rules total):\n",
      "S -> NP VP [1.0]\n",
      "NP -> Det N [0.4] | N [0.4] | Det Adj N [0.2]\n",
      "VP -> V [0.3] | V NP [0.4] | V NP PP [0.3]\n",
      "PP -> P NP [1.0]\n",
      "N -> 'time' [0.1200] | 'many' [0.0011] | 'years' [0.1050] | 'black' [0.0740] | 'over' [0.0011] | 'world' [0.0782] | 'white' [0.0054] | 'people' [0.0772] | 'water' [0.0729] | 'fish' [0.0386] | 'light' [0.0525] | 'new' [0.0343] | 'life' [0.0686] | 'old' [0.0032] | 'insects' [0.0654] | 'day' [0.0675] | 'well' [0.0032] | 'buffaloes' [0.0665] | 'animals' [0.0643] | 'first' [0.0011]\n",
      "V -> 'black' [0.0021] | 'being' [0.1408] | 'light' [0.0041] | 'fish' [0.0104] | 'insects' [0.0062] | 'found' [0.1263] | 'first' [0.0041] | 'made' [0.1180] | 'sometimes' [0.0021] | 'fly' [0.0145] | 'get' [0.0994] | 'species' [0.0021] | 'did' [0.0932] | 'colour' [0.0041] | 'make' [0.0932] | 'own' [0.0041] | 'know' [0.0890] | 'came' [0.0849] | 'seen' [0.0849] | 'change' [0.0166]\n",
      "Adj -> 'many' [0.1366] | 'like' [0.0027] | 'black' [0.0260] | 'white' [0.0929] | 'few' [0.0970] | 'such' [0.0861] | 'fish' [0.0328] | 'even' [0.0014] | 'light' [0.0191] | 'old' [0.0833] | 'new' [0.0437] | 'buffaloes' [0.0014] | 'same' [0.0820] | 'first' [0.0451] | 'great' [0.0738] | 'long' [0.0328] | 'fly' [0.0027] | 'small' [0.0683] | 'elephant' [0.0137] | 'green' [0.0587]\n",
      "Adv -> 'out' [0.0023] | 'then' [0.1020] | 'just' [0.1020] | 'over' [0.0047] | 'now' [0.0879] | 'down' [0.0317] | 'even' [0.0750] | 'well' [0.0703] | 'first' [0.0270] | 'back' [0.0469] | 'never' [0.0657] | 'long' [0.0363] | 'here' [0.0633] | 'sometimes' [0.0621] | 'before' [0.0047] | 'away' [0.0504] | 'still' [0.0574] | 'often' [0.0563] | 'too' [0.0528] | 'little' [0.0012]\n",
      "Det -> 'these' [0.2815] | 'some' [0.2706] | 'every' [0.0987] | 'those' [0.0878] | 'another' [0.0841] | 'both' [0.0768] | 'each' [0.0676] | 'either' [0.0256] | 'neither' [0.0073]\n",
      "P -> 'in' [0.15] | 'on' [0.10] | 'at' [0.08] | 'with' [0.08] | 'for' [0.10] | 'to' [0.12] | 'from' [0.07] | 'of' [0.20] | 'by' [0.05] | 'about' [0.05]\n",
      "Conj -> 'and' [0.7] | 'or' [0.2] | 'but' [0.1]\n"
     ]
    }
   ],
   "source": [
    "grammar = find_grammar_rules(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d806f-009f-4044-afbe-4c5c7ddaf607",
   "metadata": {},
   "source": [
    "The find_grammar_rules(all_text) function identifies the top 20 most frequent words in each of five POS categories: nouns, verbs, adjectives, adverbs, and determiners (top 9 for determiners due to their limited variety). The high-frequency words form the grammar's vocabulary, which increases the likelihood of generating coherent, corpus-representative sentences.\n",
    "The grammar consists of 11 rules total:\n",
    "\n",
    "5 lexical rules (N, V, Adj, Adv, Det): automatically populated with the extracted high-frequency words\n",
    "6 structural rules (S, NP, VP, PP, P, Conj): manually defined to specify sentence structure and phrase composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "858cff96-9e7e-40e5-9048-c2dcc17e3bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Words:\n",
      "\n",
      "N (Top 20):\n",
      "   1. time            (67)\n",
      "   2. buffaloes       (62)\n",
      "   3. years           (59)\n",
      "   4. insects         (56)\n",
      "   5. water           (56)\n",
      "   6. colours         (55)\n",
      "   7. flies           (55)\n",
      "   8. herd            (54)\n",
      "   9. country         (45)\n",
      "  10. light           (43)\n",
      "  11. colour          (43)\n",
      "  12. fly             (42)\n",
      "  13. animals         (41)\n",
      "  14. elephant        (41)\n",
      "  15. buffalo         (38)\n",
      "  16. head            (35)\n",
      "  17. pond            (35)\n",
      "  18. trees           (34)\n",
      "  19. day             (30)\n",
      "  20. elephants       (30)\n",
      "\n",
      "V (Top 20):\n",
      "   1. being           (53)\n",
      "   2. found           (45)\n",
      "   3. made            (31)\n",
      "   4. seen            (30)\n",
      "   5. known           (23)\n",
      "   6. killed          (23)\n",
      "   7. did             (20)\n",
      "   8. used            (18)\n",
      "   9. get             (17)\n",
      "  10. saw             (17)\n",
      "  11. make            (16)\n",
      "  12. see             (16)\n",
      "  13. came            (16)\n",
      "  14. think           (16)\n",
      "  15. produced        (15)\n",
      "  16. went            (15)\n",
      "  17. come            (15)\n",
      "  18. become          (15)\n",
      "  19. took            (14)\n",
      "  20. heard           (14)\n",
      "\n",
      "Adj (Top 20):\n",
      "   1. many            (52)\n",
      "   2. few             (44)\n",
      "   3. same            (40)\n",
      "   4. such            (38)\n",
      "   5. green           (35)\n",
      "   6. great           (33)\n",
      "   7. large           (33)\n",
      "   8. old             (29)\n",
      "   9. native          (28)\n",
      "  10. small           (27)\n",
      "  11. white           (26)\n",
      "  12. different       (22)\n",
      "  13. red             (20)\n",
      "  14. little          (19)\n",
      "  15. several         (18)\n",
      "  16. wild            (18)\n",
      "  17. own             (16)\n",
      "  18. last            (16)\n",
      "  19. insect          (16)\n",
      "  20. present         (15)\n",
      "\n",
      "Adv (Top 20):\n",
      "   1. then            (44)\n",
      "   2. now             (43)\n",
      "   3. well            (39)\n",
      "   4. just            (32)\n",
      "   5. never           (32)\n",
      "   6. often           (28)\n",
      "   7. however         (27)\n",
      "   8. almost          (26)\n",
      "   9. again           (26)\n",
      "  10. even            (24)\n",
      "  11. sometimes       (23)\n",
      "  12. away            (23)\n",
      "  13. once            (22)\n",
      "  14. thus            (21)\n",
      "  15. always          (21)\n",
      "  16. here            (20)\n",
      "  17. far             (20)\n",
      "  18. ever            (20)\n",
      "  19. soon            (20)\n",
      "  20. long            (20)\n",
      "\n",
      "Det (Top 9):\n",
      "   1. these           (89)\n",
      "   2. some            (84)\n",
      "   3. every           (32)\n",
      "   4. both            (29)\n",
      "   5. those           (25)\n",
      "   6. each            (20)\n",
      "   7. another         (19)\n",
      "   8. either          (5)\n",
      "   9. neither         (2)\n",
      "Counts of Each Category:\n",
      "  N: 7113 total, 2556 unique\n",
      "  V: 3090 total, 1375 unique\n",
      "  Adj: 2362 total, 814 unique\n",
      "  Adv: 1312 total, 305 unique\n",
      "  Det: 305 total, 9 unique\n",
      "(11 grammar rules total):\n",
      "S -> NP VP [1.0]\n",
      "NP -> Det N [0.4] | N [0.4] | Det Adj N [0.2]\n",
      "VP -> V [0.3] | V NP [0.4] | V NP PP [0.3]\n",
      "PP -> P NP [1.0]\n",
      "N -> 'time' [0.0802] | 'buffaloes' [0.0743] | 'years' [0.0707] | 'insects' [0.0671] | 'light' [0.0515] | 'water' [0.0671] | 'colours' [0.0659] | 'flies' [0.0659] | 'herd' [0.0647] | 'elephant' [0.0491] | 'fly' [0.0503] | 'colour' [0.0515] | 'country' [0.0539] | 'animals' [0.0491] | 'green' [0.0072] | 'over' [0.0012] | 'well' [0.0024] | 'buffalo' [0.0455] | 'head' [0.0419] | 'trees' [0.0407]\n",
      "V -> 'insects' [0.0106] | 'being' [0.1873] | 'fly' [0.0177] | 'colour' [0.0071] | 'found' [0.1590] | 'made' [0.1095] | 'seen' [0.1060] | 'first' [0.0071] | 'species' [0.0035] | 'rogue' [0.0035] | 'red' [0.0071] | 'known' [0.0813] | 'sometimes' [0.0035] | 'killed' [0.0813] | 'did' [0.0707] | 'blue' [0.0035] | 'hundred' [0.0141] | 'used' [0.0636] | 'cows' [0.0035] | 'get' [0.0601]\n",
      "Adj -> 'buffaloes' [0.0025] | 'light' [0.0354] | 'many' [0.1316] | 'elephant' [0.0253] | 'fly' [0.0051] | 'colour' [0.0025] | 'few' [0.1114] | 'green' [0.0886] | 'such' [0.0962] | 'same' [0.1013] | 'buffalo' [0.0025] | 'great' [0.0835] | 'like' [0.0051] | 'large' [0.0835] | 'long' [0.0304] | 'upon' [0.0025] | 'first' [0.0354] | 'animal' [0.0127] | 'native' [0.0709] | 'old' [0.0734]\n",
      "Adv -> 'then' [0.0995] | 'out' [0.0023] | 'now' [0.0973] | 'well' [0.0882] | 'over' [0.0023] | 'down' [0.0317] | 'just' [0.0724] | 'never' [0.0724] | 'long' [0.0452] | 'upon' [0.0023] | 'first' [0.0317] | 'away' [0.0520] | 'often' [0.0633] | 'however' [0.0611] | 'almost' [0.0588] | 'again' [0.0588] | 'even' [0.0543] | 'sometimes' [0.0520] | 'once' [0.0498] | 'south' [0.0045]\n",
      "Det -> 'these' [0.2918] | 'some' [0.2754] | 'every' [0.1049] | 'both' [0.0951] | 'those' [0.0820] | 'each' [0.0656] | 'another' [0.0623] | 'either' [0.0164] | 'neither' [0.0066]\n",
      "P -> 'in' [0.15] | 'on' [0.10] | 'at' [0.08] | 'with' [0.08] | 'for' [0.10] | 'to' [0.12] | 'from' [0.07] | 'of' [0.20] | 'by' [0.05] | 'about' [0.05]\n",
      "Conj -> 'and' [0.7] | 'or' [0.2] | 'but' [0.1]\n"
     ]
    }
   ],
   "source": [
    "female_grammer = find_grammar_rules(female_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbc26380-9e95-4180-9ad6-99f9156c33d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Words:\n",
      "\n",
      "N (Top 20):\n",
      "   1. black           (67)\n",
      "   2. world           (61)\n",
      "   3. people          (59)\n",
      "   4. life            (47)\n",
      "   5. time            (45)\n",
      "   6. house           (43)\n",
      "   7. years           (39)\n",
      "   8. way             (37)\n",
      "   9. birubi          (37)\n",
      "  10. food            (34)\n",
      "  11. day             (33)\n",
      "  12. climate         (32)\n",
      "  13. fish            (29)\n",
      "  14. woods           (28)\n",
      "  15. crisis          (26)\n",
      "  16. men             (25)\n",
      "  17. land            (25)\n",
      "  18. new             (24)\n",
      "  19. war             (24)\n",
      "  20. death           (24)\n",
      "\n",
      "V (Top 20):\n",
      "   1. know            (32)\n",
      "   2. want            (32)\n",
      "   3. get             (31)\n",
      "   4. got             (29)\n",
      "   5. make            (29)\n",
      "   6. don             (27)\n",
      "   7. made            (26)\n",
      "   8. did             (25)\n",
      "   9. came            (25)\n",
      "  10. take            (24)\n",
      "  11. need            (23)\n",
      "  12. see             (22)\n",
      "  13. come            (21)\n",
      "  14. say             (19)\n",
      "  15. live            (19)\n",
      "  16. used            (18)\n",
      "  17. think           (18)\n",
      "  18. went            (17)\n",
      "  19. took            (16)\n",
      "  20. told            (16)\n",
      "\n",
      "Adj (Top 20):\n",
      "   1. many            (48)\n",
      "   2. white           (42)\n",
      "   3. old             (32)\n",
      "   4. young           (29)\n",
      "   5. human           (27)\n",
      "   6. own             (27)\n",
      "   7. few             (27)\n",
      "   8. little          (27)\n",
      "   9. new             (26)\n",
      "  10. such            (25)\n",
      "  11. small           (23)\n",
      "  12. great           (21)\n",
      "  13. same            (20)\n",
      "  14. good            (20)\n",
      "  15. fish            (20)\n",
      "  16. first           (19)\n",
      "  17. next            (19)\n",
      "  18. different       (18)\n",
      "  19. wild            (17)\n",
      "  20. dead            (17)\n",
      "\n",
      "Adv (Top 20):\n",
      "   1. just            (55)\n",
      "   2. then            (43)\n",
      "   3. even            (40)\n",
      "   4. here            (34)\n",
      "   5. now             (32)\n",
      "   6. still           (31)\n",
      "   7. sometimes       (30)\n",
      "   8. too             (28)\n",
      "   9. never           (24)\n",
      "  10. back            (24)\n",
      "  11. well            (21)\n",
      "  12. once            (21)\n",
      "  13. often           (20)\n",
      "  14. away            (20)\n",
      "  15. again           (19)\n",
      "  16. almost          (19)\n",
      "  17. always          (19)\n",
      "  18. especially      (18)\n",
      "  19. later           (18)\n",
      "  20. yet             (18)\n",
      "\n",
      "Det (Top 9):\n",
      "   1. these           (65)\n",
      "   2. some            (64)\n",
      "   3. another         (27)\n",
      "   4. those           (23)\n",
      "   5. every           (22)\n",
      "   6. each            (17)\n",
      "   7. both            (13)\n",
      "   8. either          (9)\n",
      "   9. neither         (2)\n",
      "Counts of Each Category:\n",
      "  N: 7736 total, 3000 unique\n",
      "  V: 3482 total, 1485 unique\n",
      "  Adj: 2094 total, 823 unique\n",
      "  Adv: 1154 total, 273 unique\n",
      "  Det: 242 total, 9 unique\n",
      "(11 grammar rules total):\n",
      "S -> NP VP [1.0]\n",
      "NP -> Det N [0.4] | N [0.4] | Det Adj N [0.2]\n",
      "VP -> V [0.3] | V NP [0.4] | V NP PP [0.3]\n",
      "PP -> P NP [1.0]\n",
      "N -> 'black' [0.1113] | 'world' [0.1013] | 'people' [0.0980] | 'fish' [0.0482] | 'new' [0.0399] | 'many' [0.0017] | 'life' [0.0781] | 'white' [0.0050] | 'time' [0.0748] | 'house' [0.0714] | 'years' [0.0648] | 'way' [0.0615] | 'birubi' [0.0615] | 'back' [0.0100] | 'old' [0.0050] | 'climate' [0.0532] | 'food' [0.0565] | 'day' [0.0548] | 'want' [0.0017] | 'get' [0.0017]\n",
      "V -> 'black' [0.0027] | 'fish' [0.0080] | 'want' [0.0853] | 'know' [0.0853] | 'get' [0.0827] | 'don' [0.0720] | 'got' [0.0773] | 'need' [0.0613] | 'make' [0.0773] | 'own' [0.0053] | 'work' [0.0213] | 'use' [0.0373] | 'land' [0.0053] | 'made' [0.0693] | 'did' [0.0667] | 'came' [0.0667] | 'chimps' [0.0027] | 'take' [0.0640] | 'live' [0.0507] | 'see' [0.0587]\n",
      "Adj -> 'black' [0.0204] | 'fish' [0.0452] | 'new' [0.0588] | 'many' [0.1086] | 'white' [0.0950] | 'even' [0.0023] | 'old' [0.0724] | 'climate' [0.0068] | 'young' [0.0656] | 'first' [0.0430] | 'own' [0.0611] | 'human' [0.0611] | 'such' [0.0566] | 'little' [0.0611] | 'few' [0.0611] | 'forest' [0.0181] | 'small' [0.0520] | 'long' [0.0271] | 'much' [0.0362] | 'great' [0.0475]\n",
      "Adv -> 'out' [0.0025] | 'just' [0.1392] | 'over' [0.0076] | 'then' [0.1089] | 'even' [0.1013] | 'back' [0.0608] | 'here' [0.0861] | 'now' [0.0810] | 'before' [0.0051] | 'still' [0.0785] | 'too' [0.0709] | 'sometimes' [0.0759] | 'first' [0.0228] | 'down' [0.0329] | 'around' [0.0127] | 'little' [0.0025] | 'off' [0.0051] | 'never' [0.0608] | 'long' [0.0278] | 'much' [0.0177]\n",
      "Det -> 'these' [0.2686] | 'some' [0.2645] | 'another' [0.1116] | 'those' [0.0950] | 'every' [0.0909] | 'each' [0.0702] | 'both' [0.0537] | 'either' [0.0372] | 'neither' [0.0083]\n",
      "P -> 'in' [0.15] | 'on' [0.10] | 'at' [0.08] | 'with' [0.08] | 'for' [0.10] | 'to' [0.12] | 'from' [0.07] | 'of' [0.20] | 'by' [0.05] | 'about' [0.05]\n",
      "Conj -> 'and' [0.7] | 'or' [0.2] | 'but' [0.1]\n"
     ]
    }
   ],
   "source": [
    "male_grammer = find_grammar_rules(male_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4824917-4578-41b6-a319-5d486c4b3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text_to_tree(text, grammar):\n",
    "   \n",
    "    if isinstance(grammar, PCFG):\n",
    "        # PCFG needs Viterbi to find the best parse tree, finds the most probable verse\n",
    "        parser = ViterbiParser(grammar)\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(str(text))\n",
    "    all_trees = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        words = nltk.word_tokenize(sent.lower())\n",
    "        \n",
    "        try:\n",
    "            # See if our grammar can parse this\n",
    "            trees = list(parser.parse(words))\n",
    "            \n",
    "            if trees:\n",
    "                # If can, sue the parsed tree\n",
    "                tree = trees[0]\n",
    "                all_trees.append(tree)\n",
    "            else:\n",
    "                # If cannot, use create simple POS tree\n",
    "                # Parser ran fine but found zero valid parses for this sentence\n",
    "                tagged = nltk.pos_tag(words)\n",
    "                pos_tree = nltk.Tree('S', [nltk.Tree(pos, [word]) for word, pos in tagged])\n",
    "                all_trees.append(pos_tree)\n",
    "\n",
    "        # In case parser actually crashes (bad tokens, grammar issues, and so on)\n",
    "        except Exception as e:\n",
    "            # Create simple POS tree to fall back on\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            pos_tree = nltk.Tree('S', [nltk.Tree(pos, [word]) for word, pos in tagged])\n",
    "            all_trees.append(pos_tree)\n",
    "    \n",
    "    return all_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c3da5-e55d-4c0e-8207-256964ed2385",
   "metadata": {},
   "source": [
    "The parse_text_to_tree(text, grammar) takes each sentence in the text and tries to turn it into a full parse tree using your custom grammar. If the grammar can parse it, the sentence is broken into hierarchical branches like Sentence to Noun Phrase to Verb Phrase to individual words. If the grammar canâ€™t parse the sentence, the function falls back to a simpler tree made from POS tags, where each word becomes a leaf labeled with its part of speech. In the end, every sentence becomes a tree showing its grammatical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "436565aa-aa4b-4bed-b5ef-a8f4fd37a394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process samples\n",
      "  Male: 1622 trees, 35812 productions\n",
      "  Female: 1016 trees, 35379 productions\n"
     ]
    }
   ],
   "source": [
    "# Store results\n",
    "male_results = {\n",
    "    'trees': [], # actual Tree objects\n",
    "    'productions': [], # all grammar rules used (flattened across trees)\n",
    "    'depths': [], # max depth of each tree\n",
    "    'num_leaves': [] # terminal nodes (words) per tree\n",
    "}\n",
    "\n",
    "female_results = {\n",
    "    'trees': [],\n",
    "    'productions': [],\n",
    "    'depths': [],\n",
    "    'num_leaves': []\n",
    "}\n",
    "\n",
    "print(\"Process samples\")\n",
    "\n",
    "# Need to loop through every sample in the dataset in order to run through all the rules through each of the texts\n",
    "for idx, row in df.iterrows():   \n",
    "    text = row['Sample']\n",
    "    trees = parse_text_to_tree(text, grammar) # Might get multiple trees if text has multiple sentences\n",
    "    \n",
    "    for tree in trees:\n",
    "        # Extract production rules\n",
    "        productions = tree.productions()\n",
    "        \n",
    "         #Bin results by gender (0 = male, 1 = female in this dataset)\n",
    "        if row['Gender'] == 0:  \n",
    "            \n",
    "            male_results['trees'].append(tree) #\n",
    "            male_results['productions'].extend(productions)\n",
    "            male_results['depths'].append(tree.height())\n",
    "            male_results['num_leaves'].append(len(tree.leaves()))\n",
    "        # Female\n",
    "        else: \n",
    "            female_results['trees'].append(tree)\n",
    "            female_results['productions'].extend(productions)\n",
    "            female_results['depths'].append(tree.height())\n",
    "            female_results['num_leaves'].append(len(tree.leaves()))\n",
    "\n",
    "print(f\"  Male: {len(male_results['trees'])} trees, {len(male_results['productions'])} productions\")\n",
    "print(f\"  Female: {len(female_results['trees'])} trees, {len(female_results['productions'])} productions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be0e67-e4c0-474b-94fd-45f5513aed57",
   "metadata": {},
   "source": [
    "After parsing the trees through each text and storing the results, we then calculate the differences between the structure, the rules, and percentages, and display the results and differences between the rules that show up in male vs female writing. \n",
    "\n",
    "Compare production: \n",
    "- rule frequencies\n",
    "- filter out non important differences (< 0.1%)\n",
    "- compare tree depth and sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db9bd491-87f9-47d9-badd-d638615e0b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 64 differences\n",
      "\n",
      "Top 10 rules with largest gender differences:\n",
      "Rule                                               Male %   Female % Diff %  \n",
      ", -> ','                                           4.51     6.84     2.33    \n",
      ". -> '.'                                           4.35     2.82     -1.53   \n",
      "DT -> 'the'                                        5.16     6.25     1.09    \n",
      "IN -> 'of'                                         2.55     3.31     0.77    \n",
      "PRP -> 'we'                                        0.78     0.18     -0.60   \n",
      "WDT -> 'which'                                     0.11     0.70     0.59    \n",
      ": -> ';'                                           0.10     0.62     0.52    \n",
      "PRP -> 'he'                                        0.62     0.24     -0.38   \n",
      ": -> '--'                                          0.00     0.35     0.35    \n",
      "IN -> 'in'                                         1.65     2.00     0.35    \n",
      "\n",
      "Tree Structure Comparison:\n",
      "Metric                    Male            Female          Difference     \n",
      "Avg Tree Depth            3.00            3.00            0.00           \n",
      "Avg Leaves/Sentence       21.08           33.82           12.74          \n",
      "Max Tree Depth            3               3               0              \n",
      "Max Leaves/Sentence       117             159             42             \n"
     ]
    }
   ],
   "source": [
    "# Count production frequencies\n",
    "male_counter = Counter([str(p) for p in male_results['productions']])\n",
    "female_counter = Counter([str(p) for p in female_results['productions']])\n",
    "\n",
    "# Find the differences\n",
    "# Looking at every rule that is found it either corpus\n",
    "all_productions = set(male_counter.keys()).union(set(female_counter.keys()))\n",
    "differences = []\n",
    "for prod in all_productions:\n",
    "\n",
    "    # Calculate what probability of the time this rule appears in each corpus\n",
    "    male_freq = male_counter.get(prod, 0) / len(male_results['productions']) if male_results['productions'] else 0\n",
    "    female_freq = female_counter.get(prod, 0) / len(female_results['productions']) if female_results['productions'] else 0\n",
    "    \n",
    "    # If differences are smaller than .001, filter out\n",
    "    diff = female_freq - male_freq\n",
    "    if abs(diff) > 0.001: \n",
    "        differences.append({\n",
    "            'rule': prod,\n",
    "            'male': male_freq * 100,\n",
    "            'female': female_freq * 100,\n",
    "            'diff': diff * 100\n",
    "        })\n",
    "\n",
    "# Sort by absolute difference\n",
    "differences.sort(key=lambda x: abs(x['diff']), reverse=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nFound {len(differences)} differences\")\n",
    "print(\"\\nTop 10 rules with largest gender differences:\")\n",
    "print(f\"{'Rule':<50} {'Male %':<8} {'Female %':<8} {'Diff %':<8}\")\n",
    "for diff in differences[:10]:\n",
    "    print(f\"{diff['rule'][:50]:<50} {diff['male']:<8.2f} {diff['female']:<8.2f} {diff['diff']:<8.2f}\")\n",
    "\n",
    "print(\"\\nTree Structure Comparison:\")\n",
    "def calculate_stats(values):\n",
    "    if not values:\n",
    "        return 0, 0, 0\n",
    "    return np.mean(values), np.std(values), max(values)\n",
    "\n",
    "male_stats = {\n",
    "    'depth': calculate_stats(male_results['depths']),\n",
    "    'leaves': calculate_stats(male_results['num_leaves'])\n",
    "}\n",
    "female_stats = {\n",
    "    'depth': calculate_stats(female_results['depths']),\n",
    "    'leaves': calculate_stats(female_results['num_leaves'])\n",
    "}\n",
    "\n",
    "print(f\"{'Metric':<25} {'Male':<15} {'Female':<15} {'Difference':<15}\")\n",
    "print(f\"{'Avg Tree Depth':<25} {male_stats['depth'][0]:<15.2f} {female_stats['depth'][0]:<15.2f} {female_stats['depth'][0] - male_stats['depth'][0]:<15.2f}\")\n",
    "print(f\"{'Avg Leaves/Sentence':<25} {male_stats['leaves'][0]:<15.2f} {female_stats['leaves'][0]:<15.2f} {female_stats['leaves'][0] - male_stats['leaves'][0]:<15.2f}\")\n",
    "print(f\"{'Max Tree Depth':<25} {male_stats['depth'][2]:<15} {female_stats['depth'][2]:<15} {female_stats['depth'][2] - male_stats['depth'][2]:<15}\")\n",
    "print(f\"{'Max Leaves/Sentence':<25} {male_stats['leaves'][2]:<15} {female_stats['leaves'][2]:<15} {female_stats['leaves'][2] - male_stats['leaves'][2]:<15}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3] *",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
